{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596854096403",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done with the parameters reset.\nPrevious Parameters are: {'hidden_layer_sizes': [10, 50, 100], 'activation': ['identity', 'relu', 'tanh', 'logistic'], 'learning_rate': ['constant', 'invscaling', 'adaptive'], 'solver': ['lbfgs', 'sgd', 'adam']}\nCurrent Parameters are updated as: {'hidden_layer_sizes': [10], 'activation': ['relu'], 'learning_rate': ['constant'], 'solver': ['sgd']}\nDone with the parameters update.\nPrevious Parameters are: {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [0.1, 1, 10]}\nCurrent Parameters are updated as: {'C': [0.1], 'kernel': ['linear']}\nDone with the parameters update.\nPrevious Parameters are: {'n_estimators': [50, 100, 150], 'learning_rate': [0.1, 1, 10, 100]}\nCurrent Parameters are updated as: {'n_estimators': [50], 'learning_rate': [1]}\nDone with the parameters update.\nPrevious Parameters are: {'n_estimators': [5, 50, 250], 'max_depth': [2, 4, 8, 16, 32]}\nCurrent Parameters are updated as: {'n_estimators': [50], 'max_depth': [2]}\nDone with the parameters update.\nPrevious Parameters are: {'n_estimators': [50, 100, 150, 200, 250, 300], 'max_depth': [1, 3, 5, 7, 9], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4]}\nCurrent Parameters are updated as: {'n_estimators': [50], 'max_depth': [2], 'learning_rate': [1]}\nDone with the parameters update.\nPrevious Parameters are: {'n_estimators': [50, 100, 150, 200, 250, 300], 'max_depth': [3, 5, 7, 9], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4], 'verbosity': [0]}\nCurrent Parameters are updated as: {'n_estimators': [50], 'max_depth': [2], 'learning_rate': [1]}\nDone with the parameters update.\n"
    }
   ],
   "source": [
    "\n",
    "from dynapipe.utilis_func import pipeline_splitting_rule, update_parameters,reset_parameters\n",
    "reset_parameters()\n",
    "\n",
    "update_parameters(mode = \"cls\", estimator_name = \"mlp\", hidden_layer_sizes = [10],activation=[\"relu\"],learning_rate = [\"constant\"],solver = [\"sgd\"])\n",
    "update_parameters(mode = \"cls\", estimator_name = \"svm\", C=[0.1],kernel=[\"linear\"])\n",
    "update_parameters(mode = \"cls\", estimator_name = \"ada\", n_estimators =[50],learning_rate=[1])\n",
    "update_parameters(mode = \"cls\", estimator_name = \"rf\", n_estimators =[50],max_depth=[2])\n",
    "update_parameters(mode = \"cls\", estimator_name = \"gb\", n_estimators =[50],max_depth=[2],learning_rate=[1])\n",
    "update_parameters(mode = \"cls\", estimator_name = \"xgb\", n_estimators =[50],max_depth=[2],learning_rate=[1])\n",
    "\n",
    "from dynapipe.autoPipe import autoPipe\n",
    "import pandas as pd\n",
    "from dynapipe.funcPP import PPtools\n",
    "from dynapipe.autoPP import dynaPreprocessing\n",
    "\n",
    "from dynapipe.autoFS import dynaFS_clf\n",
    "from dynapipe.autoCV import evaluate_model,dynaClassifier,dynaRegressor\n",
    "df = pd.read_csv('./data/preprocessing/breast_cancer.csv')\n",
    "custom_parameters = {\n",
    "    \"scaler\" : [\"None\", \"standard\"],\n",
    "    # threshold number of category dimension\n",
    "    \"encode_band\" : [4],\n",
    "    # low dimension encoding\n",
    "    \"low_encode\" : [\"onehot\",\"label\"], \n",
    "    # high dimension encoding\n",
    "    \"high_encode\" : [\"frequency\", \"mean\"],\n",
    "    \"winsorizer\" : [(0.05,0.05),(0.1,0.1)],\n",
    "    \"sparsity\" : [0.4],\n",
    "    \"cols\" : [1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = autoPipe(\n",
    "[(\"autoPP\",dynaPreprocessing(custom_parameters = custom_parameters, label_col = 'diagnosis', model_type = \"cls\")),\n",
    "(\"datasets_splitting\",pipeline_splitting_rule(val_size = 0.2, test_size = 0.2, random_state = 13)),\n",
    "(\"autoFS\",dynaFS_clf(fs_num = 8, random_state=13, cv = 5, in_pipeline = True, input_from_file = False)),\n",
    "(\"autoCV\",dynaClassifier(random_state = 13,cv_num = 5,in_pipeline = True, input_from_file = False)),\n",
    "(\"model_evaluate\",evaluate_model(model_type = \"cls\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Now in Progress - autoFS & autoCV Iteration: Estimate about 0.0 minutes left  [####################] 100.0%\nThe top 5 Models with Best Performance Metrics:\n          Dataset Model_Name  \\\n3943  Dataset_563  mlp         \n4312  Dataset_616  lgr         \n3481  Dataset_497  mlp         \n2546  Dataset_363  gb          \n1139  Dataset_162  gb          \n\n                                                                                                                      Best_Parameters  \\\n3943  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n4312  [('C', 100), ('random_state', 13)]                                                                                                \n3481  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n2546  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n1139  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n\n      Accuracy  Precision  Recall  Latency  \n3943  0.947     0.958      0.92    3.5      \n4312  0.947     0.923      0.96    3.0      \n3481  0.930     1.000      0.84    3.0      \n2546  0.930     0.957      0.88    1.0      \n1139  0.930     0.957      0.88    2.0      \n"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns',None,'display.max_rows',None)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "DICT_PREPROCESSING,DICT_FEATURE_SELECTION,DICT_MODELS_EVALUATION,DICT_DATA,dyna_report= pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  Model_Name  Accuracy  Precision  Recall  Latency  \\\n0  lgr        0.895     0.880      0.88    3.0       \n0  svm        0.912     0.885      0.92    3.0       \n0  mlp        0.439     0.439      1.00    3.0       \n0  rf         0.877     0.821      0.92    12.0      \n0  ada        0.912     0.955      0.84    17.0      \n0  gb         0.877     0.846      0.88    3.0       \n0  xgb        0.912     0.955      0.84    2.0       \n\n                                                                                                                   Best_Parameters  \\\n0  [('C', 1000), ('random_state', 13)]                                                                                               \n0  [('C', 0.1), ('kernel', 'linear')]                                                                                                \n0  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n0  [('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                                                    \n0  [('learning_rate', 1), ('n_estimators', 50), ('random_state', 13)]                                                                \n0  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n0  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n\n     Dataset  \n0  Dataset_0  \n0  Dataset_0  \n0  Dataset_0  \n0  Dataset_0  \n0  Dataset_0  \n0  Dataset_0  \n0  Dataset_0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model_Name</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Latency</th>\n      <th>Best_Parameters</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lgr</td>\n      <td>0.895</td>\n      <td>0.880</td>\n      <td>0.88</td>\n      <td>3.0</td>\n      <td>[('C', 1000), ('random_state', 13)]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>svm</td>\n      <td>0.912</td>\n      <td>0.885</td>\n      <td>0.92</td>\n      <td>3.0</td>\n      <td>[('C', 0.1), ('kernel', 'linear')]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>mlp</td>\n      <td>0.439</td>\n      <td>0.439</td>\n      <td>1.00</td>\n      <td>3.0</td>\n      <td>[('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>rf</td>\n      <td>0.877</td>\n      <td>0.821</td>\n      <td>0.92</td>\n      <td>12.0</td>\n      <td>[('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ada</td>\n      <td>0.912</td>\n      <td>0.955</td>\n      <td>0.84</td>\n      <td>17.0</td>\n      <td>[('learning_rate', 1), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>gb</td>\n      <td>0.877</td>\n      <td>0.846</td>\n      <td>0.88</td>\n      <td>3.0</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>Dataset_0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>xgb</td>\n      <td>0.912</td>\n      <td>0.955</td>\n      <td>0.84</td>\n      <td>2.0</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>Dataset_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "\n",
    "DICT_MODELS_EVALUATION['Dataset_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          Dataset Model_Name  \\\n3943  Dataset_563  mlp         \n4312  Dataset_616  lgr         \n3481  Dataset_497  mlp         \n2546  Dataset_363  gb          \n1139  Dataset_162  gb          \n3901  Dataset_557  mlp         \n1314  Dataset_187  gb          \n1412  Dataset_201  gb          \n1678  Dataset_239  gb          \n1916  Dataset_273  gb          \n2644  Dataset_377  gb          \n2840  Dataset_405  gb          \n2851  Dataset_407  mlp         \n2994  Dataset_427  gb          \n3176  Dataset_453  gb          \n\n                                                                                                                      Best_Parameters  \\\n3943  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n4312  [('C', 100), ('random_state', 13)]                                                                                                \n3481  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n2546  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n1139  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n3901  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n1314  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n1412  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n1678  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n1916  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n2644  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n2840  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n2851  [('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]   \n2994  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n3176  [('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]                                              \n\n      Accuracy  Precision  Recall  Latency  \n3943  0.947     0.958      0.92    3.5      \n4312  0.947     0.923      0.96    3.0      \n3481  0.930     1.000      0.84    3.0      \n2546  0.930     0.957      0.88    1.0      \n1139  0.930     0.957      0.88    2.0      \n3901  0.930     0.957      0.88    2.0      \n1314  0.930     0.957      0.88    3.0      \n1412  0.930     0.957      0.88    3.0      \n1678  0.930     0.957      0.88    3.0      \n1916  0.930     0.957      0.88    3.0      \n2644  0.930     0.957      0.88    3.0      \n2840  0.930     0.957      0.88    3.0      \n2851  0.930     0.957      0.88    3.0      \n2994  0.930     0.957      0.88    3.0      \n3176  0.930     0.957      0.88    3.0      ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Model_Name</th>\n      <th>Best_Parameters</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Latency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3943</th>\n      <td>Dataset_563</td>\n      <td>mlp</td>\n      <td>[('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]</td>\n      <td>0.947</td>\n      <td>0.958</td>\n      <td>0.92</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>4312</th>\n      <td>Dataset_616</td>\n      <td>lgr</td>\n      <td>[('C', 100), ('random_state', 13)]</td>\n      <td>0.947</td>\n      <td>0.923</td>\n      <td>0.96</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3481</th>\n      <td>Dataset_497</td>\n      <td>mlp</td>\n      <td>[('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]</td>\n      <td>0.930</td>\n      <td>1.000</td>\n      <td>0.84</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2546</th>\n      <td>Dataset_363</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>Dataset_162</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3901</th>\n      <td>Dataset_557</td>\n      <td>mlp</td>\n      <td>[('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1314</th>\n      <td>Dataset_187</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1412</th>\n      <td>Dataset_201</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1678</th>\n      <td>Dataset_239</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1916</th>\n      <td>Dataset_273</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2644</th>\n      <td>Dataset_377</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2840</th>\n      <td>Dataset_405</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2851</th>\n      <td>Dataset_407</td>\n      <td>mlp</td>\n      <td>[('activation', 'relu'), ('hidden_layer_sizes', (10,)), ('learning_rate', 'constant'), ('random_state', 13), ('solver', 'sgd')]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2994</th>\n      <td>Dataset_427</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3176</th>\n      <td>Dataset_453</td>\n      <td>gb</td>\n      <td>[('learning_rate', 1), ('max_depth', 2), ('n_estimators', 50), ('random_state', 13)]</td>\n      <td>0.930</td>\n      <td>0.957</td>\n      <td>0.88</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "dyna_report.head(15)\n",
    "dyna_report.to_csv(\"dyna_report.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     concavity_mean  concave points_mean  perimeter_mean  radius_mean  \\\n264  0.09061         0.065270             111.60          17.19         \n231  0.01633         0.006588             71.76           11.32         \n197  0.11030         0.057780             117.40          18.08         \n172  0.20320         0.109700             102.50          15.46         \n54   0.05253         0.033340             97.26           15.10         \n33   0.16570         0.075930             127.90          19.27         \n68   0.25080         0.043750             60.73           9.72          \n237  0.09042         0.060220             132.50          20.48         \n51   0.01857         0.017230             87.21           13.64         \n196  0.13850         0.065260             90.63           13.77         \n\n     texture_mean  \n264  22.07         \n231  26.60         \n197  21.84         \n172  13.04         \n54   22.02         \n33   26.47         \n68   17.33         \n237  21.46         \n51   16.34         \n196  22.29         ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>perimeter_mean</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>264</th>\n      <td>0.09061</td>\n      <td>0.065270</td>\n      <td>111.60</td>\n      <td>17.19</td>\n      <td>22.07</td>\n    </tr>\n    <tr>\n      <th>231</th>\n      <td>0.01633</td>\n      <td>0.006588</td>\n      <td>71.76</td>\n      <td>11.32</td>\n      <td>26.60</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>0.11030</td>\n      <td>0.057780</td>\n      <td>117.40</td>\n      <td>18.08</td>\n      <td>21.84</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>0.20320</td>\n      <td>0.109700</td>\n      <td>102.50</td>\n      <td>15.46</td>\n      <td>13.04</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>0.05253</td>\n      <td>0.033340</td>\n      <td>97.26</td>\n      <td>15.10</td>\n      <td>22.02</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.16570</td>\n      <td>0.075930</td>\n      <td>127.90</td>\n      <td>19.27</td>\n      <td>26.47</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>0.25080</td>\n      <td>0.043750</td>\n      <td>60.73</td>\n      <td>9.72</td>\n      <td>17.33</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>0.09042</td>\n      <td>0.060220</td>\n      <td>132.50</td>\n      <td>20.48</td>\n      <td>21.46</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.01857</td>\n      <td>0.017230</td>\n      <td>87.21</td>\n      <td>13.64</td>\n      <td>16.34</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>0.13850</td>\n      <td>0.065260</td>\n      <td>90.63</td>\n      <td>13.77</td>\n      <td>22.29</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "DICT_DATA['Dataset_0']['DICT_TEST'][\"X\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"winsor_0-Scaler_None-- Encoded Features:['diagnosis', 'Size_3', 'area_mean', 'compactness_mean', 'concave points_mean', 'concavity_mean', 'fractal_dimension_mean', 'perimeter_mean', 'radius_mean', 'smoothness_mean', 'symmetry_mean', 'texture_mean', 'Frequency_Age', 'onehot_Position_1_left', 'onehot_Position_1_right', 'Frequency_Position_2', 'Frequency_Size_1', 'Frequency_Size_2', 'onehot_Treatment_no-recurrence-events', 'onehot_Treatment_recurrence-events', 'onehot_Type_1_ge40', 'onehot_Type_1_lt40', 'onehot_Type_1_premeno', 'onehot_Type_2_NaN', 'onehot_Type_2_no', 'onehot_Type_2_yes', 'onehot_Type_3_no', 'onehot_Type_3_yes']\""
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "test = DICT_PREPROCESSING['Dataset_0']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(DICT_PREPROCESSING,\"dict_preprocess\")\n",
    "save_obj(DICT_DATA,\"dict_data\")\n",
    "save_obj(DICT_MODELS_EVALUATION,\"dict_models_evaluate\")\n",
    "save_obj(dyna_report,\"dyna_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"winsor_0-Scaler_None-- Encoded Features:['diagnosis', 'Size_3', 'area_mean', 'compactness_mean', 'concave points_mean', 'concavity_mean', 'fractal_dimension_mean', 'perimeter_mean', 'radius_mean', 'smoothness_mean', 'symmetry_mean', 'texture_mean', 'Frequency_Age', 'onehot_Position_1_left', 'onehot_Position_1_right', 'Frequency_Position_2', 'Frequency_Size_1', 'Frequency_Size_2', 'onehot_Treatment_no-recurrence-events', 'onehot_Treatment_recurrence-events', 'onehot_Type_1_ge40', 'onehot_Type_1_lt40', 'onehot_Type_1_premeno', 'onehot_Type_2_NaN', 'onehot_Type_2_no', 'onehot_Type_2_yes', 'onehot_Type_3_no', 'onehot_Type_3_yes']\""
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "DICT_PREP = load_obj(\"dict_preprocess\")\n",
    "dyna_report = load_obj(\"dyna_report\")\n",
    "DICT_PREP['Dataset_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynapipe.autoViz import autoViz\n",
    "viz = autoViz(preprocess_dict=DICT_PREP,report=dyna_report)\n",
    "viz.clf_model_retrieval(metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Dataset Encode_low_dimension Encode_high_dimension    Winsorize  \\\n0  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n1  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n2  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n\n             Scale  Accuracy         Level  cnt  \n0  Scale_No Scaler  0.912     Top Accuracy  1    \n1  Scale_No Scaler  0.912     Top Accuracy  1    \n2  Scale_No Scaler  0.912     Top Accuracy  1    ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Encode_low_dimension</th>\n      <th>Encode_high_dimension</th>\n      <th>Winsorize</th>\n      <th>Scale</th>\n      <th>Accuracy</th>\n      <th>Level</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_No Scaler</td>\n      <td>0.912</td>\n      <td>Top Accuracy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_No Scaler</td>\n      <td>0.912</td>\n      <td>Top Accuracy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_No Scaler</td>\n      <td>0.912</td>\n      <td>Top Accuracy</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Dataset Encode_low_dimension Encode_high_dimension    Winsorize  \\\n0  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n1  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n2  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n\n                   Scale  Precision           Level  cnt  \n0  Scale_Scale_No Scaler  0.955      Top Precision   1    \n1  Scale_Scale_No Scaler  0.955      Top Precision   1    \n2  Scale_Scale_No Scaler  0.885      High Precision  1    ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Encode_low_dimension</th>\n      <th>Encode_high_dimension</th>\n      <th>Winsorize</th>\n      <th>Scale</th>\n      <th>Precision</th>\n      <th>Level</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.955</td>\n      <td>Top Precision</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.955</td>\n      <td>Top Precision</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.885</td>\n      <td>High Precision</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Dataset Encode_low_dimension Encode_high_dimension    Winsorize  \\\n0  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n1  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n2  Dataset_0  Low Dim_Onehot       High Dim_Frequency    Winsorize_0   \n\n                   Scale  Recall        Level  cnt  \n0  Scale_Scale_No Scaler  0.84    High Recall  1    \n1  Scale_Scale_No Scaler  0.84    High Recall  1    \n2  Scale_Scale_No Scaler  0.92    Top Recall   1    ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th>Encode_low_dimension</th>\n      <th>Encode_high_dimension</th>\n      <th>Winsorize</th>\n      <th>Scale</th>\n      <th>Recall</th>\n      <th>Level</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.84</td>\n      <td>High Recall</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.84</td>\n      <td>High Recall</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>High Dim_Frequency</td>\n      <td>Winsorize_0</td>\n      <td>Scale_Scale_No Scaler</td>\n      <td>0.92</td>\n      <td>Top Recall</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\"Dataset\",\"Encode_low_dimension\",\"Encode_high_dimension\",\"Winsorize\",\"Scale\"]\n",
    "df_pp = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in list(DICT_PREPROCESSING.keys()):\n",
    "    row_pp = [i]\n",
    "    s = DICT_PREPROCESSING[i]\n",
    "    ext = re.search(\"Encoded Features:(.*)']\", s).group(1)\n",
    "    \n",
    "    if (\"onehot_\" in ext) and (\"Frequency_\" in ext):\n",
    "        row_pp.append('Low Dim_Onehot')\n",
    "        row_pp.append('High Dim_Frequency')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"onehot_\" in ext) and (\"Mean_\" in ext):\n",
    "        row_pp.append('Low Dim_Onehot')\n",
    "        row_pp.append('High Dim_Mean')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"onehot_\" in ext) and (\"Mean_\" not in ext) and (\"Frequency_\" not in ext):\n",
    "        row_pp.append('Low Dim_Onehot')\n",
    "        row_pp.append('High Dim_No Encoder')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"Label_\" in ext) and (\"Frequency_\" in ext):\n",
    "        row_pp.append('Low Dim_Label')\n",
    "        row_pp.append('High Dim_Frequency')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"Label_\" in ext) and (\"Mean_\" in ext):\n",
    "        row_pp.append('Low Dim_Label')\n",
    "        row_pp.append('High Dim_Mean')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"Label_\" in ext) and (\"Mean_\" not in ext) and (\"Frequency_\" not in ext):\n",
    "        row_pp.append('Low Dim_Label')\n",
    "        row_pp.append('High Dim_No Encoder')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp\n",
    "    elif (\"Frequency_\" in ext) and (\"onehot_\" not in ext) and (\"Label_\" not in ext):\n",
    "        row_pp.append('Low Dim_No Encoder')\n",
    "        row_pp.append('High Dim_Frequency')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp    \n",
    "    elif (\"Mean_\" in ext) and (\"onehot_\" not in ext) and (\"Label_\" not in ext):\n",
    "        row_pp.append('Low Dim_No Encoder')\n",
    "        row_pp.append('High Dim_Mean')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp    \n",
    "    elif (\"Frequency_\" not in ext) and (\"Mean_\" not in ext) and (\"onehot_\" not in ext) and (\"Label_\" not in ext):\n",
    "        row_pp.append('Low Dim_No Encoder')\n",
    "        row_pp.append('High Dim_No Encoder')\n",
    "        row_pp.append(re.search('winsor_(.*)-Scaler', s).group(1))\n",
    "        row_pp.append(re.search('-Scaler_(.*)-- ', s).group(1))\n",
    "        df_pp.loc[len(df_pp)] = row_pp    \n",
    "\n",
    "\n",
    "df_report_Accuracy = df_pp.merge(dyna_report[['Dataset','Accuracy']], how = 'left', on = 'Dataset')\n",
    "bins = [0, 0.70, 0.90, 1]\n",
    "labels = [\"Low Accuracy\",\"High Accuracy\",\"Top Accuracy\"]\n",
    "df_report_Accuracy['Level'] = pd.cut(df_report_Accuracy['Accuracy'], bins=bins, labels=labels)\n",
    "df_report_Accuracy['cnt'] = 1\n",
    "df_report_Accuracy.loc[df_report_Accuracy['Scale'] == 'None','Scale'] = \"No Scaler\"\n",
    "df_report_Accuracy['Scale'] = 'Scale_'+df_report_Accuracy['Scale']\n",
    "df_report_Accuracy['Winsorize'] = 'Winsorize_' + df_report_Accuracy['Winsorize']\n",
    "df_report_Accuracy.head(3)\n",
    "\n",
    "df_report_Precision = df_pp.merge(dyna_report[['Dataset','Precision']], how = 'left', on = 'Dataset')\n",
    "bins = [0, 0.70, 0.90, 1]\n",
    "labels = [\"Low Precision\",\"High Precision\",\"Top Precision\"]\n",
    "df_report_Precision['Level'] = pd.cut(df_report_Precision['Precision'], bins=bins, labels=labels)\n",
    "df_report_Precision['cnt'] = 1\n",
    "df_report_Precision.loc[df_report_Precision['Scale'] == 'None','Scale'] = \"No Scaler\"\n",
    "df_report_Precision['Scale'] = 'Scale_'+df_report_Accuracy['Scale']\n",
    "df_report_Precision['Winsorize'] = 'Winsorize_' + df_report_Precision['Winsorize']\n",
    "df_report_Precision.head(3)\n",
    "\n",
    "df_report_Recall = df_pp.merge(dyna_report[['Dataset','Recall']], how = 'left', on = 'Dataset')\n",
    "bins = [0, 0.70, 0.90, 1]\n",
    "labels = [\"Low Recall\",\"High Recall\",\"Top Recall\"]\n",
    "df_report_Recall['Level'] = pd.cut(df_report_Recall['Recall'], bins=bins, labels=labels)\n",
    "df_report_Recall['cnt'] = 1\n",
    "df_report_Recall.loc[df_report_Recall['Scale'] == 'None','Scale'] = \"No Scaler\"\n",
    "df_report_Recall['Scale'] = 'Scale_'+df_report_Accuracy['Scale']\n",
    "df_report_Recall['Winsorize'] = 'Winsorize_' + df_report_Recall['Winsorize']\n",
    "df_report_Recall.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  antecedentIndex consequentIndex  Total\n0  Dataset_0       Low Dim_Onehot  7.0  \n1  Dataset_1       Low Dim_Onehot  7.0  \n2  Dataset_10      Low Dim_Onehot  7.0  \n3  Dataset_100     Low Dim_Onehot  7.0  \n4  Dataset_101     Low Dim_Onehot  7.0  \n5  Dataset_102     Low Dim_Onehot  7.0  \n6  Dataset_103     Low Dim_Onehot  7.0  \n7  Dataset_104     Low Dim_Onehot  7.0  \n8  Dataset_105     Low Dim_Onehot  7.0  \n9  Dataset_106     Low Dim_Onehot  7.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antecedentIndex</th>\n      <th>consequentIndex</th>\n      <th>Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dataset_0</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dataset_1</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dataset_10</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dataset_100</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dataset_101</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Dataset_102</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Dataset_103</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Dataset_104</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Dataset_105</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Dataset_106</td>\n      <td>Low Dim_Onehot</td>\n      <td>7.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "step1_df = df_report_Accuracy.groupby(['Encode_low_dimension','Dataset'], as_index=False)['cnt'].count().rename({\"cnt\":\"Total\",\"Dataset\":\"antecedentIndex\",\"Encode_low_dimension\":\"consequentIndex\"},axis = 1)[['antecedentIndex','consequentIndex','Total']]\n",
    "step2_df = df_report_Accuracy.groupby(['Encode_low_dimension','Encode_high_dimension'], as_index=False)['cnt'].count().rename({\"cnt\":\"Total\",\"Encode_low_dimension\":\"antecedentIndex\",\"Encode_high_dimension\":\"consequentIndex\"},axis = 1)[['antecedentIndex','consequentIndex','Total']]\n",
    "step3_df = df_report_Accuracy.groupby(['Encode_high_dimension','Winsorize'], as_index=False)['cnt'].count().rename({\"cnt\":\"Total\",\"Encode_high_dimension\":\"antecedentIndex\",\"Winsorize\":\"consequentIndex\"},axis = 1)[['antecedentIndex','consequentIndex','Total']]\n",
    "step4_df = df_report_Accuracy.groupby(['Winsorize','Scale'], as_index=False)['cnt'].count().rename({\"cnt\":\"Total\",\"Winsorize\":\"antecedentIndex\",\"Scale\":\"consequentIndex\"},axis = 1)[['antecedentIndex','consequentIndex','Total']]\n",
    "step5_df = df_report_Accuracy.groupby(['Scale','Level'], as_index=False)['cnt'].count().rename({\"cnt\":\"Total\",\"Scale\":\"antecedentIndex\",\"Level\":\"consequentIndex\"},axis = 1)[['antecedentIndex','consequentIndex','Total']].dropna()\n",
    "integrated_df = pd.concat([step1_df,step2_df,step3_df,step4_df,step5_df],axis = 0)\n",
    "integrated_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(integrated_df['antecedentIndex'].append(integrated_df['consequentIndex']).drop_duplicates(),columns = {\"label\"})\n",
    "label_df['Number'] = label_df.reset_index().index\n",
    "label_list = list(label_df.label)\n",
    "\n",
    "\n",
    "\n",
    "source_df = pd.DataFrame(integrated_df['antecedentIndex'])\n",
    "source_df = source_df.merge(label_df, left_on=['antecedentIndex'], right_on = ['label'],how = 'left')\n",
    "source_list = list(source_df['Number'])\n",
    "\n",
    "\n",
    "target_df = pd.DataFrame(integrated_df['consequentIndex'])\n",
    "target_df = target_df.merge(label_df, left_on=['consequentIndex'], right_on = ['label'],how = 'left')\n",
    "target_list = list(target_df['Number'])\n",
    "\n",
    "value_list = [int(i) for i in list(integrated_df.Total)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "link": {
          "source": [
           0,
           1,
           2,
           3,
           4,
           5,
           6,
           7,
           8,
           9,
           10,
           11,
           12,
           13,
           14,
           15,
           16,
           17,
           18,
           19,
           20,
           21,
           22,
           23,
           24,
           25,
           26,
           27,
           28,
           29,
           30,
           31,
           32,
           33,
           34,
           35,
           36,
           37,
           38,
           39,
           40,
           41,
           42,
           43,
           44,
           45,
           46,
           47,
           48,
           49,
           50,
           51,
           52,
           53,
           54,
           55,
           56,
           57,
           58,
           59,
           60,
           61,
           62,
           63,
           64,
           65,
           66,
           67,
           68,
           69,
           70,
           71,
           72,
           73,
           74,
           75,
           76,
           77,
           78,
           79,
           80,
           81,
           82,
           83,
           84,
           85,
           86,
           87,
           88,
           89,
           90,
           91,
           92,
           93,
           94,
           95,
           96,
           97,
           98,
           99,
           100,
           101,
           102,
           103,
           104,
           105,
           106,
           107,
           108,
           109,
           110,
           111,
           112,
           113,
           114,
           115,
           116,
           117,
           118,
           119,
           120,
           121,
           122,
           123,
           124,
           125,
           126,
           127,
           128,
           129,
           130,
           131,
           132,
           133,
           134,
           135,
           136,
           137,
           138,
           139,
           140,
           141,
           142,
           143,
           144,
           145,
           146,
           147,
           148,
           149,
           150,
           151,
           152,
           153,
           154,
           155,
           156,
           157,
           158,
           159,
           160,
           161,
           162,
           163,
           164,
           165,
           166,
           167,
           168,
           169,
           170,
           171,
           172,
           173,
           174,
           175,
           176,
           177,
           178,
           179,
           180,
           181,
           182,
           183,
           184,
           185,
           186,
           187,
           188,
           189,
           190,
           191,
           192,
           193,
           194,
           195,
           196,
           197,
           198,
           199,
           200,
           201,
           202,
           203,
           204,
           205,
           206,
           207,
           208,
           209,
           210,
           211,
           212,
           213,
           214,
           215,
           216,
           217,
           218,
           219,
           220,
           221,
           222,
           223,
           224,
           225,
           226,
           227,
           228,
           229,
           230,
           231,
           232,
           233,
           234,
           235,
           236,
           237,
           238,
           239,
           240,
           241,
           242,
           243,
           244,
           245,
           246,
           247,
           248,
           249,
           250,
           251,
           252,
           253,
           254,
           255,
           256,
           257,
           258,
           259,
           260,
           261,
           262,
           263,
           264,
           265,
           266,
           267,
           268,
           269,
           270,
           271,
           272,
           273,
           274,
           275,
           276,
           277,
           278,
           279,
           280,
           281,
           282,
           283,
           284,
           285,
           286,
           287,
           288,
           289,
           290,
           291,
           292,
           293,
           294,
           295,
           296,
           297,
           298,
           299,
           300,
           301,
           302,
           303,
           304,
           305,
           306,
           307,
           308,
           309,
           310,
           311,
           312,
           313,
           314,
           315,
           316,
           317,
           318,
           319,
           320,
           321,
           322,
           323,
           324,
           325,
           326,
           327,
           328,
           329,
           330,
           331,
           332,
           333,
           334,
           335,
           336,
           337,
           338,
           339,
           340,
           341,
           342,
           343,
           344,
           345,
           346,
           347,
           348,
           349,
           350,
           351,
           352,
           353,
           354,
           355,
           356,
           357,
           358,
           359,
           360,
           361,
           362,
           363,
           364,
           365,
           366,
           367,
           368,
           369,
           370,
           371,
           372,
           373,
           374,
           375,
           376,
           377,
           378,
           379,
           380,
           381,
           382,
           383,
           384,
           385,
           386,
           387,
           388,
           389,
           390,
           391,
           392,
           393,
           394,
           395,
           396,
           397,
           398,
           399,
           400,
           401,
           402,
           403,
           404,
           405,
           406,
           407,
           408,
           409,
           410,
           411,
           412,
           413,
           414,
           415,
           416,
           417,
           418,
           419,
           420,
           421,
           422,
           423,
           424,
           425,
           426,
           427,
           428,
           429,
           430,
           431,
           432,
           433,
           434,
           435,
           436,
           437,
           438,
           439,
           440,
           441,
           442,
           443,
           444,
           445,
           446,
           447,
           448,
           449,
           450,
           451,
           452,
           453,
           454,
           455,
           456,
           457,
           458,
           459,
           460,
           461,
           462,
           463,
           464,
           465,
           466,
           467,
           468,
           469,
           470,
           471,
           472,
           473,
           474,
           475,
           476,
           477,
           478,
           479,
           480,
           481,
           482,
           483,
           484,
           485,
           486,
           487,
           488,
           489,
           490,
           491,
           492,
           493,
           494,
           495,
           496,
           497,
           498,
           499,
           500,
           501,
           502,
           503,
           504,
           505,
           506,
           507,
           508,
           509,
           510,
           511,
           512,
           513,
           514,
           515,
           516,
           517,
           518,
           519,
           520,
           521,
           522,
           523,
           524,
           525,
           526,
           527,
           528,
           529,
           530,
           531,
           532,
           533,
           534,
           535,
           536,
           537,
           538,
           539,
           540,
           541,
           542,
           543,
           544,
           545,
           546,
           547,
           548,
           549,
           550,
           551,
           552,
           553,
           554,
           555,
           556,
           557,
           558,
           559,
           560,
           561,
           562,
           563,
           564,
           565,
           566,
           567,
           568,
           569,
           570,
           571,
           572,
           573,
           574,
           575,
           576,
           577,
           578,
           579,
           580,
           581,
           582,
           583,
           584,
           585,
           586,
           587,
           588,
           589,
           590,
           591,
           592,
           593,
           594,
           595,
           596,
           597,
           598,
           599,
           600,
           601,
           602,
           603,
           604,
           605,
           606,
           607,
           608,
           609,
           610,
           611,
           612,
           613,
           614,
           615,
           616,
           617,
           618,
           619,
           620,
           621,
           622,
           623,
           624,
           625,
           626,
           627,
           628,
           629,
           630,
           631,
           632,
           633,
           634,
           635,
           636,
           637,
           638,
           639,
           640,
           641,
           642,
           643,
           644,
           645,
           646,
           647,
           648,
           649,
           650,
           651,
           652,
           653,
           654,
           655,
           656,
           657,
           658,
           659,
           660,
           661,
           662,
           663,
           664,
           665,
           666,
           667,
           668,
           669,
           670,
           671,
           672,
           673,
           674,
           675,
           676,
           677,
           678,
           679,
           680,
           681,
           682,
           683,
           684,
           685,
           686,
           687,
           688,
           689,
           690,
           691,
           692,
           693,
           694,
           695,
           696,
           697,
           698,
           699,
           700,
           701,
           702,
           703,
           704,
           704,
           705,
           705,
           706,
           706,
           707,
           707,
           708,
           708,
           709,
           709,
           709,
           710,
           710
          ],
          "target": [
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           704,
           705,
           706,
           707,
           708,
           707,
           708,
           709,
           710,
           709,
           710,
           711,
           712,
           713,
           712,
           713
          ],
          "value": [
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           7,
           4620,
           308,
           2310,
           2310,
           154,
           154,
           1232,
           1232,
           1232,
           1232,
           317,
           1300,
           847,
           1451,
           1013
          ]
         },
         "node": {
          "color": "rgb(71,172,55)",
          "label": [
           "Dataset_0",
           "Dataset_1",
           "Dataset_10",
           "Dataset_100",
           "Dataset_101",
           "Dataset_102",
           "Dataset_103",
           "Dataset_104",
           "Dataset_105",
           "Dataset_106",
           "Dataset_107",
           "Dataset_108",
           "Dataset_109",
           "Dataset_11",
           "Dataset_110",
           "Dataset_111",
           "Dataset_112",
           "Dataset_113",
           "Dataset_114",
           "Dataset_115",
           "Dataset_116",
           "Dataset_117",
           "Dataset_118",
           "Dataset_119",
           "Dataset_12",
           "Dataset_120",
           "Dataset_121",
           "Dataset_122",
           "Dataset_123",
           "Dataset_124",
           "Dataset_125",
           "Dataset_126",
           "Dataset_127",
           "Dataset_128",
           "Dataset_129",
           "Dataset_13",
           "Dataset_130",
           "Dataset_131",
           "Dataset_132",
           "Dataset_133",
           "Dataset_134",
           "Dataset_135",
           "Dataset_136",
           "Dataset_137",
           "Dataset_138",
           "Dataset_139",
           "Dataset_14",
           "Dataset_140",
           "Dataset_141",
           "Dataset_142",
           "Dataset_143",
           "Dataset_144",
           "Dataset_145",
           "Dataset_146",
           "Dataset_147",
           "Dataset_148",
           "Dataset_149",
           "Dataset_15",
           "Dataset_150",
           "Dataset_151",
           "Dataset_152",
           "Dataset_153",
           "Dataset_154",
           "Dataset_155",
           "Dataset_156",
           "Dataset_157",
           "Dataset_158",
           "Dataset_159",
           "Dataset_16",
           "Dataset_160",
           "Dataset_161",
           "Dataset_162",
           "Dataset_163",
           "Dataset_164",
           "Dataset_165",
           "Dataset_166",
           "Dataset_167",
           "Dataset_168",
           "Dataset_169",
           "Dataset_17",
           "Dataset_170",
           "Dataset_171",
           "Dataset_172",
           "Dataset_173",
           "Dataset_174",
           "Dataset_175",
           "Dataset_176",
           "Dataset_177",
           "Dataset_178",
           "Dataset_179",
           "Dataset_18",
           "Dataset_180",
           "Dataset_181",
           "Dataset_182",
           "Dataset_183",
           "Dataset_184",
           "Dataset_185",
           "Dataset_186",
           "Dataset_187",
           "Dataset_188",
           "Dataset_189",
           "Dataset_19",
           "Dataset_190",
           "Dataset_191",
           "Dataset_192",
           "Dataset_193",
           "Dataset_194",
           "Dataset_195",
           "Dataset_196",
           "Dataset_197",
           "Dataset_198",
           "Dataset_199",
           "Dataset_2",
           "Dataset_20",
           "Dataset_200",
           "Dataset_201",
           "Dataset_202",
           "Dataset_203",
           "Dataset_204",
           "Dataset_205",
           "Dataset_206",
           "Dataset_207",
           "Dataset_208",
           "Dataset_209",
           "Dataset_21",
           "Dataset_210",
           "Dataset_211",
           "Dataset_212",
           "Dataset_213",
           "Dataset_214",
           "Dataset_215",
           "Dataset_216",
           "Dataset_217",
           "Dataset_218",
           "Dataset_219",
           "Dataset_22",
           "Dataset_220",
           "Dataset_221",
           "Dataset_222",
           "Dataset_223",
           "Dataset_224",
           "Dataset_225",
           "Dataset_226",
           "Dataset_227",
           "Dataset_228",
           "Dataset_229",
           "Dataset_23",
           "Dataset_230",
           "Dataset_231",
           "Dataset_232",
           "Dataset_233",
           "Dataset_234",
           "Dataset_235",
           "Dataset_236",
           "Dataset_237",
           "Dataset_238",
           "Dataset_239",
           "Dataset_24",
           "Dataset_240",
           "Dataset_241",
           "Dataset_242",
           "Dataset_243",
           "Dataset_244",
           "Dataset_245",
           "Dataset_246",
           "Dataset_247",
           "Dataset_248",
           "Dataset_249",
           "Dataset_25",
           "Dataset_250",
           "Dataset_251",
           "Dataset_252",
           "Dataset_253",
           "Dataset_254",
           "Dataset_255",
           "Dataset_256",
           "Dataset_257",
           "Dataset_258",
           "Dataset_259",
           "Dataset_26",
           "Dataset_260",
           "Dataset_261",
           "Dataset_262",
           "Dataset_263",
           "Dataset_264",
           "Dataset_265",
           "Dataset_266",
           "Dataset_267",
           "Dataset_268",
           "Dataset_269",
           "Dataset_27",
           "Dataset_270",
           "Dataset_271",
           "Dataset_272",
           "Dataset_273",
           "Dataset_274",
           "Dataset_275",
           "Dataset_276",
           "Dataset_277",
           "Dataset_278",
           "Dataset_279",
           "Dataset_28",
           "Dataset_280",
           "Dataset_281",
           "Dataset_282",
           "Dataset_283",
           "Dataset_284",
           "Dataset_285",
           "Dataset_286",
           "Dataset_287",
           "Dataset_288",
           "Dataset_289",
           "Dataset_29",
           "Dataset_290",
           "Dataset_291",
           "Dataset_292",
           "Dataset_293",
           "Dataset_294",
           "Dataset_295",
           "Dataset_296",
           "Dataset_297",
           "Dataset_298",
           "Dataset_299",
           "Dataset_3",
           "Dataset_30",
           "Dataset_300",
           "Dataset_301",
           "Dataset_302",
           "Dataset_303",
           "Dataset_304",
           "Dataset_305",
           "Dataset_306",
           "Dataset_307",
           "Dataset_308",
           "Dataset_309",
           "Dataset_31",
           "Dataset_310",
           "Dataset_311",
           "Dataset_312",
           "Dataset_313",
           "Dataset_314",
           "Dataset_315",
           "Dataset_316",
           "Dataset_317",
           "Dataset_318",
           "Dataset_319",
           "Dataset_32",
           "Dataset_320",
           "Dataset_321",
           "Dataset_322",
           "Dataset_323",
           "Dataset_324",
           "Dataset_325",
           "Dataset_326",
           "Dataset_327",
           "Dataset_328",
           "Dataset_329",
           "Dataset_33",
           "Dataset_330",
           "Dataset_331",
           "Dataset_332",
           "Dataset_333",
           "Dataset_334",
           "Dataset_335",
           "Dataset_336",
           "Dataset_337",
           "Dataset_338",
           "Dataset_339",
           "Dataset_34",
           "Dataset_340",
           "Dataset_341",
           "Dataset_342",
           "Dataset_343",
           "Dataset_344",
           "Dataset_345",
           "Dataset_346",
           "Dataset_347",
           "Dataset_348",
           "Dataset_349",
           "Dataset_35",
           "Dataset_350",
           "Dataset_351",
           "Dataset_352",
           "Dataset_353",
           "Dataset_354",
           "Dataset_355",
           "Dataset_356",
           "Dataset_357",
           "Dataset_358",
           "Dataset_359",
           "Dataset_36",
           "Dataset_360",
           "Dataset_361",
           "Dataset_362",
           "Dataset_363",
           "Dataset_364",
           "Dataset_365",
           "Dataset_366",
           "Dataset_367",
           "Dataset_368",
           "Dataset_369",
           "Dataset_37",
           "Dataset_370",
           "Dataset_371",
           "Dataset_372",
           "Dataset_373",
           "Dataset_374",
           "Dataset_375",
           "Dataset_376",
           "Dataset_377",
           "Dataset_378",
           "Dataset_379",
           "Dataset_38",
           "Dataset_380",
           "Dataset_381",
           "Dataset_382",
           "Dataset_383",
           "Dataset_384",
           "Dataset_385",
           "Dataset_386",
           "Dataset_387",
           "Dataset_388",
           "Dataset_389",
           "Dataset_39",
           "Dataset_390",
           "Dataset_391",
           "Dataset_392",
           "Dataset_393",
           "Dataset_394",
           "Dataset_395",
           "Dataset_396",
           "Dataset_397",
           "Dataset_398",
           "Dataset_399",
           "Dataset_4",
           "Dataset_40",
           "Dataset_400",
           "Dataset_401",
           "Dataset_402",
           "Dataset_403",
           "Dataset_404",
           "Dataset_405",
           "Dataset_406",
           "Dataset_407",
           "Dataset_408",
           "Dataset_409",
           "Dataset_41",
           "Dataset_410",
           "Dataset_411",
           "Dataset_412",
           "Dataset_413",
           "Dataset_414",
           "Dataset_415",
           "Dataset_416",
           "Dataset_417",
           "Dataset_418",
           "Dataset_419",
           "Dataset_42",
           "Dataset_420",
           "Dataset_421",
           "Dataset_422",
           "Dataset_423",
           "Dataset_424",
           "Dataset_425",
           "Dataset_426",
           "Dataset_427",
           "Dataset_428",
           "Dataset_429",
           "Dataset_43",
           "Dataset_430",
           "Dataset_431",
           "Dataset_432",
           "Dataset_433",
           "Dataset_434",
           "Dataset_435",
           "Dataset_436",
           "Dataset_437",
           "Dataset_438",
           "Dataset_439",
           "Dataset_44",
           "Dataset_440",
           "Dataset_441",
           "Dataset_442",
           "Dataset_443",
           "Dataset_444",
           "Dataset_445",
           "Dataset_446",
           "Dataset_447",
           "Dataset_448",
           "Dataset_449",
           "Dataset_45",
           "Dataset_450",
           "Dataset_451",
           "Dataset_452",
           "Dataset_453",
           "Dataset_454",
           "Dataset_455",
           "Dataset_456",
           "Dataset_457",
           "Dataset_458",
           "Dataset_459",
           "Dataset_46",
           "Dataset_460",
           "Dataset_461",
           "Dataset_462",
           "Dataset_463",
           "Dataset_464",
           "Dataset_465",
           "Dataset_466",
           "Dataset_467",
           "Dataset_468",
           "Dataset_469",
           "Dataset_47",
           "Dataset_470",
           "Dataset_471",
           "Dataset_472",
           "Dataset_473",
           "Dataset_474",
           "Dataset_475",
           "Dataset_476",
           "Dataset_477",
           "Dataset_478",
           "Dataset_479",
           "Dataset_48",
           "Dataset_480",
           "Dataset_481",
           "Dataset_482",
           "Dataset_483",
           "Dataset_484",
           "Dataset_485",
           "Dataset_486",
           "Dataset_487",
           "Dataset_488",
           "Dataset_489",
           "Dataset_49",
           "Dataset_490",
           "Dataset_491",
           "Dataset_492",
           "Dataset_493",
           "Dataset_494",
           "Dataset_495",
           "Dataset_496",
           "Dataset_497",
           "Dataset_498",
           "Dataset_499",
           "Dataset_5",
           "Dataset_50",
           "Dataset_500",
           "Dataset_501",
           "Dataset_502",
           "Dataset_503",
           "Dataset_504",
           "Dataset_505",
           "Dataset_506",
           "Dataset_507",
           "Dataset_508",
           "Dataset_509",
           "Dataset_51",
           "Dataset_510",
           "Dataset_511",
           "Dataset_512",
           "Dataset_513",
           "Dataset_514",
           "Dataset_515",
           "Dataset_516",
           "Dataset_517",
           "Dataset_518",
           "Dataset_519",
           "Dataset_52",
           "Dataset_520",
           "Dataset_521",
           "Dataset_522",
           "Dataset_523",
           "Dataset_524",
           "Dataset_525",
           "Dataset_526",
           "Dataset_527",
           "Dataset_528",
           "Dataset_529",
           "Dataset_53",
           "Dataset_530",
           "Dataset_531",
           "Dataset_532",
           "Dataset_533",
           "Dataset_534",
           "Dataset_535",
           "Dataset_536",
           "Dataset_537",
           "Dataset_538",
           "Dataset_539",
           "Dataset_54",
           "Dataset_540",
           "Dataset_541",
           "Dataset_542",
           "Dataset_543",
           "Dataset_544",
           "Dataset_545",
           "Dataset_546",
           "Dataset_547",
           "Dataset_548",
           "Dataset_549",
           "Dataset_55",
           "Dataset_550",
           "Dataset_551",
           "Dataset_552",
           "Dataset_553",
           "Dataset_554",
           "Dataset_555",
           "Dataset_556",
           "Dataset_557",
           "Dataset_558",
           "Dataset_559",
           "Dataset_56",
           "Dataset_560",
           "Dataset_561",
           "Dataset_562",
           "Dataset_563",
           "Dataset_564",
           "Dataset_565",
           "Dataset_566",
           "Dataset_567",
           "Dataset_568",
           "Dataset_569",
           "Dataset_57",
           "Dataset_570",
           "Dataset_571",
           "Dataset_572",
           "Dataset_573",
           "Dataset_574",
           "Dataset_575",
           "Dataset_576",
           "Dataset_577",
           "Dataset_578",
           "Dataset_579",
           "Dataset_58",
           "Dataset_580",
           "Dataset_581",
           "Dataset_582",
           "Dataset_583",
           "Dataset_584",
           "Dataset_585",
           "Dataset_586",
           "Dataset_587",
           "Dataset_588",
           "Dataset_589",
           "Dataset_59",
           "Dataset_590",
           "Dataset_591",
           "Dataset_592",
           "Dataset_593",
           "Dataset_594",
           "Dataset_595",
           "Dataset_596",
           "Dataset_597",
           "Dataset_598",
           "Dataset_599",
           "Dataset_6",
           "Dataset_60",
           "Dataset_600",
           "Dataset_601",
           "Dataset_602",
           "Dataset_603",
           "Dataset_604",
           "Dataset_605",
           "Dataset_606",
           "Dataset_607",
           "Dataset_608",
           "Dataset_609",
           "Dataset_61",
           "Dataset_610",
           "Dataset_611",
           "Dataset_612",
           "Dataset_613",
           "Dataset_614",
           "Dataset_615",
           "Dataset_616",
           "Dataset_617",
           "Dataset_618",
           "Dataset_619",
           "Dataset_62",
           "Dataset_620",
           "Dataset_621",
           "Dataset_622",
           "Dataset_623",
           "Dataset_624",
           "Dataset_625",
           "Dataset_626",
           "Dataset_627",
           "Dataset_628",
           "Dataset_629",
           "Dataset_63",
           "Dataset_630",
           "Dataset_631",
           "Dataset_632",
           "Dataset_633",
           "Dataset_634",
           "Dataset_635",
           "Dataset_636",
           "Dataset_637",
           "Dataset_638",
           "Dataset_639",
           "Dataset_64",
           "Dataset_640",
           "Dataset_641",
           "Dataset_642",
           "Dataset_643",
           "Dataset_644",
           "Dataset_645",
           "Dataset_646",
           "Dataset_647",
           "Dataset_648",
           "Dataset_649",
           "Dataset_65",
           "Dataset_650",
           "Dataset_651",
           "Dataset_652",
           "Dataset_653",
           "Dataset_654",
           "Dataset_655",
           "Dataset_656",
           "Dataset_657",
           "Dataset_658",
           "Dataset_659",
           "Dataset_66",
           "Dataset_660",
           "Dataset_661",
           "Dataset_662",
           "Dataset_663",
           "Dataset_664",
           "Dataset_665",
           "Dataset_666",
           "Dataset_667",
           "Dataset_668",
           "Dataset_669",
           "Dataset_67",
           "Dataset_670",
           "Dataset_671",
           "Dataset_672",
           "Dataset_673",
           "Dataset_674",
           "Dataset_675",
           "Dataset_676",
           "Dataset_677",
           "Dataset_678",
           "Dataset_679",
           "Dataset_68",
           "Dataset_680",
           "Dataset_681",
           "Dataset_682",
           "Dataset_683",
           "Dataset_684",
           "Dataset_685",
           "Dataset_686",
           "Dataset_687",
           "Dataset_688",
           "Dataset_689",
           "Dataset_69",
           "Dataset_690",
           "Dataset_691",
           "Dataset_692",
           "Dataset_693",
           "Dataset_694",
           "Dataset_695",
           "Dataset_696",
           "Dataset_697",
           "Dataset_698",
           "Dataset_699",
           "Dataset_7",
           "Dataset_70",
           "Dataset_700",
           "Dataset_701",
           "Dataset_702",
           "Dataset_703",
           "Dataset_71",
           "Dataset_72",
           "Dataset_73",
           "Dataset_74",
           "Dataset_75",
           "Dataset_76",
           "Dataset_77",
           "Dataset_78",
           "Dataset_79",
           "Dataset_8",
           "Dataset_80",
           "Dataset_81",
           "Dataset_82",
           "Dataset_83",
           "Dataset_84",
           "Dataset_85",
           "Dataset_86",
           "Dataset_87",
           "Dataset_88",
           "Dataset_89",
           "Dataset_9",
           "Dataset_90",
           "Dataset_91",
           "Dataset_92",
           "Dataset_93",
           "Dataset_94",
           "Dataset_95",
           "Dataset_96",
           "Dataset_97",
           "Dataset_98",
           "Dataset_99",
           "Low Dim_Onehot",
           "High Dim_Frequency",
           "High Dim_Mean",
           "Winsorize_0",
           "Winsorize_1",
           "Scale_No Scaler",
           "Scale_standard",
           "Low Accuracy",
           "High Accuracy",
           "Top Accuracy"
          ],
          "line": {
           "color": "rgb(25,100,90)",
           "width": 0.5
          },
          "pad": 15,
          "thickness": 10
         },
         "type": "sankey"
        }
       ],
       "layout": {
        "font": {
         "size": 8
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Pipeline Cluster Traversal Experiments - autoViz Model Retrieval Diagram <a href=\"https://www.linkedin.com/in/lei-tony-dong/\"> ©Tony Dong</a>"
        }
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'temp-plot.html'"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 10,\n",
    "      line = dict(color = 'rgb(25,100,90)', width = 0.5),\n",
    "      label = label_list,\n",
    "      color = 'rgb(71,172,55)'\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = source_list, \n",
    "      target = target_list,\n",
    "      value = value_list\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title = 'Pipeline Cluster Traversal Experiments - autoViz Model Retrieval Diagram <a href=\"https://www.linkedin.com/in/lei-tony-dong/\"> ©Tony Dong</a>', font_size=8)\n",
    "from plotly.offline import plot\n",
    "plot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "'pip' is not recognized as an internal or external command,\noperable program or batch file.\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('learning_rate', 1),\n ('max_depth', 2),\n ('n_estimators', 50),\n ('random_state', 13)]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "a = {'learning_rate': 1, 'max_depth': 2, 'n_estimators': 50, 'random_state': 13}\n",
    "lis = a.items()\n",
    "[i for i in lis]"
   ]
  }
 ]
}